---
phase: 03-core-grading-experience
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/api/app.py
  - src/api/schemas.py
  - src/analysis/pre_analysis.py
autonomous: true
requirements:
  - GRAD-01
  - GRAD-02
user_setup: []

must_haves:
  truths:
    - "User can upload multiple PDF copies via POST /api/sessions/{session_id}/upload"
    - "Pre-analysis endpoint returns detected grading scale and student names"
    - "Grading scale is editable via confirm endpoint before grading starts"
    - "Multiple scales detected are presented with confidence scores"
    - "PDF validation rejects invalid files with clear error messages"
  artifacts:
    - path: "src/api/app.py"
      provides: "POST /upload, POST /pre-analyze, POST /confirm-pre-analysis endpoints"
      exports: ["upload_copies", "pre_analyze_session", "confirm_pre_analysis"]
    - path: "src/api/schemas.py"
      provides: "Pydantic models for pre-analysis requests/responses"
      contains: "PreAnalysisRequest, PreAnalysisResponse, ConfirmPreAnalysisRequest"
    - path: "src/analysis/pre_analysis.py"
      provides: "PreAnalyzer class for scale detection"
      contains: "class PreAnalyzer"
  key_links:
    - from: "src/api/app.py"
      to: "src/analysis/pre_analysis.py"
      via: "import PreAnalyzer"
      pattern: "from analysis.pre_analysis import PreAnalyzer"
    - from: "src/api/app.py"
      to: "src/api/schemas.py"
      via: "Pydantic validation"
      pattern: "PreAnalysisRequest, PreAnalysisResponse"
    - from: "src/analysis/pre_analysis.py"
      to: "core.models"
      via: "PreAnalysisResult model"
      pattern: "PreAnalysisResult"
---

<objective>
Implement PDF upload workflow with automatic grading scale detection and teacher confirmation.

This plan creates the complete upload-to-grade-start flow: upload PDFs, detect grading scale via vision AI, present results for teacher confirmation/adjustment, and freeze the scale before grading begins.

Purpose: Establish the pre-grading workflow that validates PDF structure and captures the grading scale, preventing costly LLM grading runs with incorrect scales.

Output: Upload endpoint, pre-analysis endpoint, scale confirmation endpoint, request/response schemas.
</objective>

<execution_context>
@/home/olivier/.claude/get-shit-done/workflows/execute-plan.md
@/home/olivier/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/REQUIREMENTS.md
@.planning/phases/03-core-grading-experience/03-CONTEXT.md
@.planning/phases/03-core-grading-experience/03-RESEARCH.md
@src/api/app.py
@src/api/schemas.py
@src/analysis/pre_analysis.py
@src/core/models.py

<interfaces>
<!-- From src/api/app.py - Existing patterns to follow -->
Existing endpoints already use:
- `current_user = Depends(get_current_user)` for authentication
- `SessionStore(session_id, user_id=user_id)` for multi-tenant data isolation
- `BackgroundTasks` for async operations
- Rate limiting via `@limiter.limit()`

<!-- From src/api/schemas.py - Existing schema patterns -->
```python
from pydantic import BaseModel, Field
from typing import List, Dict, Optional, Literal

class CreateSessionRequest(BaseModel):
    subject: Optional[str] = None
    topic: Optional[str] = None
    question_weights: Optional[Dict[str, float]] = None
```

<!-- From src/analysis/pre_analysis.py - PreAnalyzer usage -->
```python
class PreAnalyzer:
    def __init__(self, user_id: str, session_id: str, language: str = "fr"):
        # Initializes with user-scoped storage

    def analyze(self, pdf_path: str, force_refresh: bool = False) -> PreAnalysisResult:
        # Returns detected grading_scale, students, blocking_issues

class PreAnalysisResult(BaseModel):
    grading_scale: Dict[str, float]  # {"Q1": 3.0, "Q2": 5.0}
    students: List[StudentInfo]  # detected students
    blocking_issues: List[str]
    has_blocking_issues: bool
```
</interfaces>
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add multi-scale detection to PreAnalyzer</name>
  <files>src/analysis/pre_analysis.py</files>
  <action>
Extend PreAnalyzer._parse_response() to handle multiple grading scale detections:

1. When AI returns multiple candidate scales (e.g., due to ambiguous document structure), parse all scales with confidence scores
2. Store scales as list of dicts: `[{"scale": {"Q1": 3.0}, "confidence": 0.85}, {"scale": {"Q1": 4.0}, "confidence": 0.62}]`
3. Add field to PreAnalysisResult: `candidate_scales: List[Dict[str, Any]] = Field(default_factory=list)`
4. Update pre-analysis prompt (in pre_analysis_prompts.py) to request multiple scales when uncertain: `"grading_scales": [{"scale": {...}, "confidence": 0.8}, ...]`
5. Maintain backward compatibility: if single scale returned, store as first candidate with confidence_grading_scale

Do NOT modify the analyze() method signature - only internal parsing and result structure.
</action>
  <verify>
grep -n "candidate_scales" src/analysis/pre_analysis.py | head -5
</verify>
  <done>
PreAnalysisResult includes candidate_scales list with confidence scores for each detected scale option.
</done>
</task>

<task type="auto">
  <name>Task 2: Add pre-analysis request/response schemas</name>
  <files>src/api/schemas.py</files>
  <action>
Add Pydantic schemas for pre-analysis workflow:

```python
class PreAnalysisRequest(BaseModel):
    force_refresh: bool = False
    """Re-run analysis even if cached (tier-based limits apply)"""

class CandidateScale(BaseModel):
    scale: Dict[str, float] = Field(default_factory=dict)
    confidence: float = 0.0

class PreAnalysisResponse(BaseModel):
    analysis_id: str
    is_valid_pdf: bool
    page_count: int
    document_type: str
    confidence_document_type: float
    structure: str
    subject_integration: str
    num_students_detected: int
    students: List["StudentInfoSchema"] = Field(default_factory=list)
    grading_scale: Dict[str, float] = Field(default_factory=dict)
    confidence_grading_scale: float
    candidate_scales: List[CandidateScale] = Field(default_factory=list)
    questions_detected: List[str] = Field(default_factory=list)
    blocking_issues: List[str] = Field(default_factory=list)
    has_blocking_issues: bool
    warnings: List[str] = Field(default_factory=list)
    quality_issues: List[str] = Field(default_factory=list)
    overall_quality_score: float
    detected_language: str
    cached: bool
    analysis_duration_ms: float

class ConfirmPreAnalysisRequest(BaseModel):
    adjustments: Optional[Dict[str, Any]] = None
    """Teacher adjustments: {"grading_scale": {"Q1": 4.0}, "student_names": {0: "Dupont"}}"""
    selected_scale_index: Optional[int] = None
    """If multiple scales detected, which one to use (0-based)"""

class ConfirmPreAnalysisResponse(BaseModel):
    success: bool
    session_id: str
    status: str
    grading_scale: Dict[str, float]
    num_students: int
```

Import these from core.models where they already exist to avoid duplication:
- StudentInfoSchema (if exists, otherwise define)
- Other schemas may already exist - check first before adding new ones
</action>
  <verify>
grep -n "PreAnalysisRequest\|PreAnalysisResponse\|ConfirmPreAnalysisRequest" src/api/schemas.py | head -10
</verify>
  <done>
Pydantic schemas define request/response contracts for pre-analysis endpoints.
</done>
</task>

<task type="auto">
  <name>Task 3: Complete upload and pre-analysis endpoints</name>
  <files>src/api/app.py</files>
  <action>
The upload endpoint already exists. Enhance it for GRAD-01 (batch upload):

1. Update POST /api/sessions/{session_id}/upload to support multiple files (already works via `files: List[UploadFile]`)
2. Add validation: max 50 PDFs per batch limit (create constant MAX_BATCH_SIZE = 50)
3. Return individual file validation results for each PDF

Enhance POST /api/sessions/{session_id}/pre-analyze:
1. Already exists - verify it returns candidate_scales in response (from Task 1)
2. If force_refresh=True and tier is FREE, return 403 with message (already implemented)
3. Ensure student names are included in response.students list

Verify POST /api/sessions/{session_id}/confirm-pre-analysis:
1. Apply selected_scale_index if provided: use the scale from candidate_scales[selected_scale_index]
2. Apply adjustments.grading_scale overrides to selected scale
3. Apply adjustments.student_names to override detected names (store in session.metadata)
4. Set session.policy.question_weights to final scale
5. Set session.status = "ready_for_grading"
6. Save session via store.save_session()

All endpoints must use:
- `current_user = Depends(get_current_user)` for auth
- `SessionStore(session_id, user_id=user_id)` for data isolation
- Proper 404 responses when session not found
</action>
  <verify>
# Test endpoint registration
curl -s http://localhost:8000/docs | grep -o "pre-analyze\|confirm-pre-analysis" | head -5
</verify>
  <done>
Upload accepts multiple PDFs, pre-analysis returns detected scales with confidence, confirmation endpoint freezes the scale for grading.
</done>
</task>

</tasks>

<verification>
1. Start API: `uvicorn src.api.app:app --reload`
2. Create session: `curl -X POST http://localhost:8000/api/sessions -H "Authorization: Bearer $TOKEN"`
3. Upload test PDF: `curl -X POST http://localhost:8000/api/sessions/{id}/upload -F "files=@test.pdf" -H "Authorization: Bearer $TOKEN"`
4. Run pre-analysis: `curl -X POST http://localhost:8000/api/sessions/{id}/pre-analyze -H "Authorization: Bearer $TOKEN"`
5. Verify response includes grading_scale, candidate_scales (if multiple), students list
6. Confirm scale: `curl -X POST http://localhost:8000/api/sessions/{id}/confirm-pre-analysis -d '{"adjustments": {"grading_scale": {"Q1": 4.0}}}' -H "Authorization: Bearer $TOKEN"`
7. Verify session status is "ready_for_grading" and question_weights are set
</verification>

<success_criteria>
1. POST /upload accepts up to 50 PDF files and validates each
2. POST /pre-analyze returns grading_scale (best match) and candidate_scales list with confidence scores
3. POST /confirm-pre-analysis applies teacher adjustments and freezes scale in session.policy.question_weights
4. All endpoints enforce multi-tenant data isolation via user_id
5. Invalid PDFs return clear error messages without crashing
</success_criteria>

<output>
After completion, create `.planning/phases/03-core-grading-experience/03-01-SUMMARY.md` with:
- Changes made to PreAnalyzer for multi-scale detection
- New/updated schemas in schemas.py
- Endpoint behaviors and request/response formats
</output>
